------------------------------------------------------------------------------------------------------------------------
JAX - Basics
------------------------------------------------------------------------------------------------------------------------
JAX is a library used for functional programming. It stands for Just another python library for XLA (accelerated linear algebra).
JAX is used for fast computation of mathematical functions.
It is similar to NumPy.
All the numpy function can be replaced with jax function.
Along with that, jax also provides auto-differentiation.
One major difference is that jax arrays are immutable.
It is heavily used for GPU computation.
It also uses the JIT compilation.
There are four major transformation functions in jax. they are grad (for auto-differentiation), jit (for just in time
    compilation), vmap(for vectorization), pmap(for parallel computation).
    grad: Auto differentiation. It takes the derivative of a function using reverse mode differentiation.
        Reverse mode differentiation is a method of computing derivatives by repeatedly applying the chain rule to the
        partial derivatives.
    jit: When you jit a function, jax traces teh operations and hands them tot he XLA compiler. XLA performs the
        operation fusion by combining the multiple steps into one to save memory bandwidth and optimizes the execution
        for the specific hardware.
    vmap: Instead of manually writing loops to process a batch of data, vmap automatically vectorizes the function. It
        pushes the loop down into the library level, which is faster than a python 'for' loop.
    pmap: This allows to map a function across multiple devices for lightning - fast distributed training.
Unlike numpy, jax uses a global state for random numbers generation. Jax needs a state to be passed explicitly.
------------------------------------------------------------------------------------------------------------------------
Numpy vs JAX vs PyTorch
------------------------------------------------------------------------------------------------------------------------
import numpy as np

# Creation
x = np.array([1, 2, 3])
x = np.zeros((2, 3))
x = np.ones_like(another_array)
x = np.arange(0, 10, 2)
x = np.linspace(0, 1, 5)

# Manipulation
x = np.reshape(x, (5, 1))
x = np.transpose(x)
x = np.concatenate([x, y], axis=0)
x = np.stack([x, y], axis=1)
x = np.expand_dims(x, axis=0)
x = np.squeeze(x)

# Math & Reductions
z = np.add(x, y)      # or x + y
z = np.matmul(x, y)   # or x @ y
res = np.sum(x, axis=0)
res = np.mean(x)
res = np.argmax(x, axis=-1)

# Mutation (In-place)
x[0] = 10

# Random
np.random.seed(42)
r = np.random.normal(0, 1, size=(3, 3))

# Boolean Masking & Searching
mask = x > 0.5
values = x[mask]
indices = np.where(x > 0.5)
clamped = np.clip(x, min_val, max_val)

# Advanced Indexing & Broadcasting
row_indices = np.array([0, 1])
col_indices = np.array([1, 2])
selected = x[row_indices, col_indices]
x_new_axis = x[:, np.newaxis] # Add dimension for broadcasting

# Sorting & Searching
sorted_x = np.sort(x, axis=0)
sorted_idx = np.argsort(x)
top_k_idx = np.argpartition(x, -k)[-k:]

# Linear Algebra (linalg)
norm = np.linalg.norm(x)
inv = np.linalg.inv(matrix)
eigenvalues, eigenvectors = np.linalg.eig(matrix)
solution = np.linalg.solve(A, b)

# Data Persistence
np.save('data.npy', x)
x = np.load('data.npy')
------------------------------------------------------------------------------------------------------------------------
import jax
import jax.numpy as jnp
from jax import lax

# Creation (Same as NumPy)
x = jnp.array([1, 2, 3])
x = jnp.zeros((2, 3))
x = jnp.arange(0, 10, 2)

# Manipulation (Same as NumPy)
x = jnp.reshape(x, (5, 1))
x = jnp.concatenate([x, y], axis=0)
x = jnp.expand_dims(x, axis=0)

# Math & Reductions (Same as NumPy)
z = jnp.dot(x, y)
res = jnp.sum(x, axis=0)
res = jnp.argmax(x, axis=-1)

# Mutation (Functional/Immutable - returns a NEW array)
# You cannot do x[0] = 10
x = x.at[0].set(10)
x = x.at[1].add(5)

# Random (Explicit State Management)
key = jax.random.PRNGKey(42)
key, subkey = jax.random.split(key)
r = jax.random.normal(subkey, shape=(3, 3))

# Transformations (Unique to JAX)
f_jit = jax.jit(my_func)
f_vmap = jax.vmap(my_func)
f_grad = jax.grad(my_func)

# Boolean Masking (Standard)
indices = jnp.where(x > 0.5)
clamped = jnp.clip(x, min_val, max_val)

# Advanced Indexing (Must use .at for updates)
# To gather values:
values = x[jnp.array([0, 2, 4])]

# Structured Control Flow (Crucial for @jit)
# Standard 'if' and 'for' often fail inside jit; use these:
result = lax.cond(pred, true_fun, false_fun, operand)
result = lax.fori_loop(lower, upper, body_fun, init_val)
result = lax.while_loop(cond_fun, body_fun, init_val)

# Parallelism (Unique to JAX)
# Map a function across multiple GPUs/TPUs
parallel_results = jax.pmap(my_func)(batch_of_data)

# Low-level XLA ops (jax.lax)
# Faster, less overhead than jnp for some operations
z = lax.add(x, y)
z = lax.dot(matrix1, matrix2)
------------------------------------------------------------------------------------------------------------------------
import torch
import torch.nn.functional as F

# Creation
x = torch.tensor([1, 2, 3])
x = torch.zeros((2, 3))
x = torch.ones_like(another_tensor)
x = torch.arange(0, 10, 2)

# Manipulation
x = x.view(5, 1)          # Preferred over reshape
x = x.t()                 # Transpose
x = torch.cat([x, y], dim=0)
x = torch.stack([x, y], dim=1)
x = torch.unsqueeze(x, dim=0)
x = torch.squeeze(x)

# Math & Reductions
z = torch.add(x, y)       # or x + y
z = torch.matmul(x, y)    # or x @ y
res = torch.sum(x, dim=0)
res = torch.mean(x)
res = torch.argmax(x, dim=-1)

# Mutation (In-place)
x[0] = 10
x.add_(5)                 # Underscore denotes in-place

# Device Management
x = x.to('cuda')          # Move to GPU
x = x.to('cpu')           # Move to CPU

# Random
torch.manual_seed(42)
r = torch.randn(3, 3)

# Boolean Masking & Searching
mask = x > 0.5
values = torch.masked_select(x, mask)
indices = torch.where(x > 0.5)
clamped = torch.clamp(x, min=0, max=1)

# Advanced Indexing (Gather/Scatter)
# Useful for NLP and RL to pick specific indices from a batch
gathered = torch.gather(x, dim=1, index=indices)
# Update x at indices with values (in-place)
x.scatter_(dim=1, index=indices, src=values)

# Top-K (Very common in PyTorch)
values, indices = torch.topk(x, k=5, dim=-1)

# Autograd (The heart of PyTorch)
x = torch.randn(3, requires_grad=True)
y = x * 2
y.sum().backward()
print(x.grad) # Access the calculated gradient

# Neural Network Functionals (torch.nn.functional)
# Instead of layers, use raw functions
probs = F.softmax(logits, dim=-1)
loss = F.cross_entropy(logits, targets)
activated = F.relu(x)

# Serialization
torch.save(model.state_dict(), 'model.pth')
------------------------------------------------------------------------------------------------------------------------
Grain
------------------------------------------------------------------------------------------------------------------------
Key Features:
Flexibility for highly customized data preparation workflows.
Deterministic for reproducibility, debugging, and consistent experimental results.
Resilient to preemptions for easy checkpointing.

* It is a CPU based data processing. It prepares data in batches and processes them in parallel and supply it to the
    accelerators. Not every system is suitable for this but this is the default setting in Grain.
------------------------------------------------------------------------------------------------------------------------
DataLoader: Class
* This has three abstractions. DataSource, Sampler, Transformations.
* This will automatically manages the launching and managing of the child procesess to parallelize the processing of input
    data, (including sharding, shuffling), collection output and providing the batched data for the model consumption.
data_loader = grain.DataLoader(
    data_source = source,
    sampler = index_sampler,
    operations = transformations,
    worker_count = 4)

Grain DataSources:
1. ArrayRecord:
        data_source = ArrayRecord(paths = ['data.arrayrecord']
2. Parquet:
        data_source = Parquet(path = ['data.parquet'])
3. TFDatasets:
        data_source = tfds.data_source('imagenet_a', split= "train")
4. Custom DataSource:
   * First we need to ensure that the data source is pickleable. The data sources will be pickled and sent off to child
        processes.
   * Close file handlers- create data source as context manager that closes its open file handles.

Grain Samplers:
1. IndexSampler:
        sampler = grain.IndexSampler(
        num_records = 20, # limits the results
        num_epochs = 2,
        shard_options = grain.ShardOptions(
                            shard_index = 0, shard_count = 1,
                            drop_remainder = True),
        shuffle = True,
        seed = 42
        )
for record_metadata in sampler:
    print(record_metadata)

Grain Transformations:
1. Map:
    class PlusOne(grain.transforms.Map):
        def map(self, x: int) -> int:
            return x + 1
    plus_one = PlusOne()
2. Filter: transforms the dictionaries to lists
    class FlatMapTransformer(grain.experimental.FlatMapTransform):
        def flat_map(self, x):
            for val in x:
                yield val

    class OnlyWordElements(grain.transforms.Filter):
        def filter(self,element: int)-> bool:
            word = 'on'
            return word in element
    word_filter = OnlyWordElements()
3. Batch:
    batching = grain.transforms.Batch(batch_size = 4, drop_remainder = True)

DataLoader brings it all together.
    transformations = [plus_one, word_filter, words, batching]
    data_loader = grain.DataLoader(data_source = source,
    sampler = index_sampler,
    operations = transformations,
    worker_count = 4)

    for x in data_loader:
        print(x)
------------------------------------------------------------------------------------------------------------------------
Dataset API: class
    this utilizes a chaining syntax to create a data transformation steps.
    provides more general type of transformation tools like dataset mixing, and great control over executions.
    It preserves random access capabilities even after transformations.
    Used to debug the data elements at specific positions without processing the entire dataset.

dataset = grain.MapDataset(source([0,1,2,3,4,5,6,7])
            .shuffle(seed=42)
            .map(lambda x:x*10)
            .batch(batch_size = 2)
print(list(dataset))

------------------------------------------------------------------------------------------------------------------------
Grain Data sources:
Same datasources as in data loaders

Grain Data Transformations:
    Shuffle, Map,Filter, to_iter_dataset, Batch
* we need to convert the dataset to "to_iter_dataset" if we have to use the batching after filter. this converts the data
   to an iterator rather than the random acess dataset.


dataset = (
    grain_source
    .shuffle(seed=42)
    .map(lambda x:x['title'].decode('utf-8'))
    .filter(lambda x: "pandas" in x.lower())
    .to_iter_dataset()
    .batch(batch_size = 20)
print(list(dataset))

Grain has compatibility with Orbax for the data checkpointing.
import orbax.checkpoin as ocp
mngr = ocp.CheckpointManager(checkpoint_dir = "checkpoint_dir")
mngr.save(
        step = 1, arge = grain.checkpoint.CheckpointSave(dataset_iter),
        force = True)

mngr.wait_until_finished()
